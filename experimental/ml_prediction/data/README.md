# Data Directory

This directory is for storing stock data used in training and evaluation.

## Two Ways to Provide Data

### Option 1: Google Sheets (Recommended)
Load stock data directly from Google Sheets URLs - no need to save local files!

See the `load_from_google_sheets.sh` script and `stocks_config_example.txt` for examples.

### Option 2: Local CSV Files
Save individual stock CSV files in the `stocks/` directory.

## Directory Structure

```
data/
├── stocks/              # (Optional) Place individual stock CSV files here
│   ├── AAPL.csv
│   ├── MSFT.csv
│   ├── GOOGL.csv
│   └── ... (more stocks)
└── combined_stocks.csv  # Generated by aggregate_data.py
```

## Stock CSV Format

Each stock CSV file should have the following format:

```csv
Date,Open,High,Low,Close,Volume
1/2/2015 17:30:00,71.43,71.68,71.05,71.33,611903
1/5/2015 17:30:00,71.2,71.28,70.6,70.75,410586
1/7/2015 17:30:00,69.55,69.6,67.5,68.4,2140213
...
```

### Requirements

- **Date column**: Any parseable date format
- **OHLCV columns**: Open, High, Low, Close, Volume
- **Column names**: Case-insensitive (Open/OPEN/open all work)
- **Minimum rows**: 250+ (more is better, ideally 2000+)
- **Date range**: Longer history improves model generalization

### Supported Column Name Variations

The aggregation script handles various naming conventions:
- Date: `Date`, `DATE`, `date`
- Open: `Open`, `OPEN`, `open`
- High: `High`, `HIGH`, `high`
- Low: `Low`, `LOW`, `low`
- Close: `Close`, `CLOSE`, `close`, `Close*`, `Adj Close`
- Volume: `Volume`, `VOLUME`, `volume`, `Vol.`

## Usage

### Option A: Using Google Sheets

**Step 1**: Set up your Google Sheets
- Create a sheet with OHLCV data
- Each stock can be on a different tab (gid parameter)
- Make sure it's publicly accessible or shareable

**Step 2**: Get the export URL
Format: `https://docs.google.com/spreadsheets/d/SHEET_ID/export?format=csv&gid=TAB_NUMBER`
- `gid=0` is the first tab
- `gid=1` is the second tab, etc.

**Step 3**: Run aggregation with URLs
```bash
cd experimental/ml_prediction

python aggregate_data.py --google_sheets \
    "AAPL=https://docs.google.com/spreadsheets/d/.../export?format=csv&gid=0" \
    "MSFT=https://docs.google.com/spreadsheets/d/.../export?format=csv&gid=1" \
    --output_file data/combined_stocks.csv
```

**Or edit and run the convenience script**:
```bash
./load_from_google_sheets.sh  # Runs entire workflow automatically
```

### Option B: Using Local Files

**Step 1**: Add stock files to `stocks/` directory
```bash
# Example structure
experimental/ml_prediction/data/stocks/
├── AAPL.csv      # Apple Inc.
├── MSFT.csv      # Microsoft
├── GOOGL.csv     # Google
├── AMZN.csv      # Amazon
└── TSLA.csv      # Tesla
```

**Step 2**: Run aggregation from directory
```bash
cd experimental/ml_prediction
python aggregate_data.py --input_dir data/stocks --output_file data/combined_stocks.csv
```

### Result

Both methods create `data/combined_stocks.csv` with all stocks combined and a `stock_id` column added.

### 3. Train Model

Use the combined data for training:

```bash
python train_multi_stock.py --data_file data/combined_stocks.csv --model_name my_model
```

## Tips

- **Diversity**: Use stocks from different sectors for better generalization
- **Quality**: Ensure data is clean (no missing dates, anomalies checked)
- **Quantity**: More stocks = better model (5-10 minimum, 20+ ideal)
- **History**: Longer time series = more training examples (aim for 2000+ rows per stock)

## Example: Downloading Data

You can download historical stock data from various sources:

- **Yahoo Finance**: https://finance.yahoo.com/
- **Alpha Vantage**: https://www.alphavantage.co/
- **IEX Cloud**: https://iexcloud.io/
- **Quandl**: https://www.quandl.com/

Most providers offer CSV export in OHLCV format.
